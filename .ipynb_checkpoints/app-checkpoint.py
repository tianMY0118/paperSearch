"""
paperSearch - æ™ºèƒ½å­¦æœ¯æ–‡çŒ®åŠ©æ‰‹ v1.0
Author: tianMY0118
Description: ä» arXiv å¿«é€Ÿæ£€ç´¢å¹¶å¯¼å‡ºå­¦æœ¯è®ºæ–‡ï¼Œæ”¯æŒå¤šæ ¼å¼è¾“å‡ºã€‚
License: MIT
"""

import gradio as gr
import requests
import feedparser
import sys
import io
import logging
from datetime import datetime
import os
import json

# ===== ç¬¬ä¸‰æ–¹åº“ =====
from docx import Document
import pandas as pd
from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas

# ===== æ—¥å¿—é…ç½® =====
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [ScholarSift] %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger("ScholarSift")

# ===== å…¨å±€çŠ¶æ€ç¼“å­˜ =====
paper_data_cache = []
current_query_info = {}

# ä¿®å¤ stdout ç¼–ç ï¼ˆå…¼å®¹ Windowsï¼‰
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# ===== æ ¸å¿ƒåŠŸèƒ½ =====
def search_papers(query: str, max_results: int):
    global paper_data_cache, current_query_info
    logger.info(f"ç”¨æˆ·å‘èµ·æ£€ç´¢ï¼šå…³é”®è¯='{query}', æœ€å¤šè¿”å› {max_results} ç¯‡")
    
    try:
        arxiv_url = f"http://export.arxiv.org/api/query?search_query=all:{query}&start=0&max_results={max_results}"
        response = requests.get(arxiv_url, timeout=15)
        response.raise_for_status()
    except Exception as e:
        logger.error(f"arXiv API è¯·æ±‚å¤±è´¥: {e}")
        return "âŒ æ— æ³•è¿æ¥ arXiv æœåŠ¡ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–ç¨åé‡è¯•ã€‚", None

    feed = feedparser.parse(response.text)
    if not feed.entries:
        logger.warning("æœªæ‰¾åˆ°åŒ¹é…è®ºæ–‡")
        return "ğŸ” æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡ã€‚", None

    papers = []
    output = f"ğŸ“š ScholarSift æ£€ç´¢ç»“æœï¼ˆå…³é”®è¯: {query}ï¼‰\n\n"
    for idx, entry in enumerate(feed.entries):
        title = entry.title
        authors = ', '.join(author.name for author in entry.authors)
        published = entry.published.split("T")[0]
        abstract = entry.summary.replace('\n', ' ').strip()
        pdf_link = entry.links[1].href if len(entry.links) > 1 else entry.id

        papers.append({
            "Title": title,
            "Authors": authors,
            "Published": published,
            "PDF Link": pdf_link,
            "Abstract": abstract,
        })

        output += f"ğŸ“„ è®ºæ–‡ {idx + 1}\n"
        output += f"æ ‡é¢˜       : {title}\n"
        output += f"ä½œè€…       : {authors}\n"
        output += f"å‘è¡¨æ—¥æœŸ   : {published}\n"
        output += f"PDF é“¾æ¥   : {pdf_link}\n"
        output += f"æ‘˜è¦       : {abstract}\n"
        output += "â€”" * 60 + "\n\n"

    paper_data_cache = papers
    current_query_info = {
        "query": query,
        "max_results": max_results,
        "num_found": len(papers),
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }

    logger.info(f"æ£€ç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")
    return output, gr.update(visible=True)


def export_results(format: str):
    global paper_data_cache
    logger.info(f"ç”¨æˆ·è¯·æ±‚å¯¼å‡ºä¸º {format} æ ¼å¼")
    
    papers = paper_data_cache
    if not papers:
        logger.warning("å¯¼å‡ºå¤±è´¥ï¼šæ— ç¼“å­˜æ•°æ®")
        return None

    os.makedirs("outputs", exist_ok=True)
    base_name = "scholarsift_export"
    ext_map = {"Text": "txt", "Word": "docx", "PDF": "pdf", "Excel": "xlsx"}
    filename = f"outputs/{base_name}.{ext_map.get(format, 'txt')}"

    watermark = "\nâ€” å¯¼å‡ºè‡ª ScholarSift æ™ºèƒ½å­¦æœ¯åŠ©æ‰‹ (https://yourwebsite.com) â€”\n"

    try:
        if format == "Word":
            doc = Document()
            doc.add_heading("ScholarSift å­¦æœ¯è®ºæ–‡å¯¼å‡ºæŠ¥å‘Š", 0)
            for i, p in enumerate(papers):
                doc.add_heading(f"è®ºæ–‡ {i+1}: {p['Title']}", level=1)
                doc.add_paragraph(f"ä½œè€…       : {p['Authors']}")
                doc.add_paragraph(f"å‘è¡¨æ—¥æœŸ   : {p['Published']}")
                doc.add_paragraph(f"PDF é“¾æ¥   : {p['PDF Link']}")
                doc.add_paragraph(f"æ‘˜è¦       : {p['Abstract']}")
            doc.add_paragraph(watermark)
            doc.save(filename)

        elif format == "PDF":
            c = canvas.Canvas(filename, pagesize=letter)
            width, height = letter
            margin = 50
            y = height - margin

            def draw_line(text, size=10, spacing=14):
                nonlocal y
                c.setFont("Helvetica", size)
                for line in text.split('\n'):
                    if y < margin:
                        c.showPage()
                        y = height - margin
                        c.setFont("Helvetica", size)
                    c.drawString(margin, y, line[:100])  # é˜²æ­¢è¶…å®½
                    y -= spacing

            draw_line("ScholarSift å­¦æœ¯è®ºæ–‡å¯¼å‡ºæŠ¥å‘Š", size=14, spacing=20)
            y -= 10
            for i, p in enumerate(papers):
                draw_line(f"è®ºæ–‡ {i+1}: {p['Title']}", size=12)
                draw_line(f"ä½œè€…       : {p['Authors']}")
                draw_line(f"å‘è¡¨æ—¥æœŸ   : {p['Published']}")
                draw_line(f"PDF é“¾æ¥   : {p['PDF Link']}")
                draw_line(f"æ‘˜è¦       : {p['Abstract']}")
                draw_line("â€”" * 70)
                y -= 10
            draw_line(watermark)
            c.save()

        elif format == "Excel":
            df = pd.DataFrame(papers)
            df.to_excel(filename, index=False)

        else:  # Text
            with open(filename, 'w', encoding='utf-8') as f:
                f.write("ScholarSift å­¦æœ¯è®ºæ–‡å¯¼å‡ºæŠ¥å‘Š\n\n")
                for i, p in enumerate(papers):
                    f.write(f"è®ºæ–‡ {i+1}\n")
                    for k, v in p.items():
                        f.write(f"{k:<12}: {v}\n")
                    f.write("â€”" * 60 + "\n\n")
                f.write(watermark)

        # è®°å½•å¯¼å‡ºæ—¥å¿—ï¼ˆå¯é€‰ï¼‰
        log_event = {
            "tool": "ScholarSift",
            "action": "export",
            "format": format,
            "count": len(papers),
            "time": datetime.now().isoformat()
        }
        with open("scholarsift_export_log.json", "w", encoding="utf-8") as f:
            json.dump(log_event, f, ensure_ascii=False, indent=2)

        logger.info(f"å¯¼å‡ºæˆåŠŸ: {filename}")
        return filename

    except Exception as e:
        logger.error(f"å¯¼å‡ºå¤±è´¥: {e}")
        return None


# ===== Gradio ç•Œé¢ =====
with gr.Blocks(title="ScholarSift") as demo:
    gr.Markdown("## ğŸ§  ScholarSift â€” æ™ºèƒ½å­¦æœ¯æ–‡çŒ®åŠ©æ‰‹")
    gr.Markdown("å¿«é€Ÿä» arXiv æ£€ç´¢å‰æ²¿è®ºæ–‡ï¼Œå¹¶ä¸€é”®å¯¼å‡ºä¸º Word / PDF / Excel / æ–‡æœ¬æ ¼å¼")

    with gr.Row():
        topic = gr.Textbox(
            label="ç ”ç©¶ä¸»é¢˜å…³é”®è¯",
            placeholder="ä¾‹å¦‚ï¼šå¤§è¯­è¨€æ¨¡å‹ã€é‡å­è®¡ç®—ã€æ°”å€™å˜åŒ–",
            value="Large Language Models"
        )
        num = gr.Slider(1, 10, value=5, step=1, label="æœ€å¤šè¿”å›ç¯‡æ•°")

    search_btn = gr.Button("ğŸ” å¼€å§‹æ£€ç´¢", variant="primary")
    result_box = gr.Textbox(label="æ£€ç´¢ç»“æœ", lines=20, max_lines=30)
    
    with gr.Row():
        export_dropdown = gr.Dropdown(
            choices=["Text", "Word", "PDF", "Excel"],
            label="é€‰æ‹©å¯¼å‡ºæ ¼å¼",
            value="PDF",
            interactive=True
        )
        export_btn = gr.Button("ğŸ“¤ å¯¼å‡ºç»“æœ", variant="secondary")

    file_output = gr.File(label="ğŸ“¥ ä¸‹è½½å¯¼å‡ºæ–‡ä»¶")

    # äº‹ä»¶ç»‘å®š
    search_btn.click(
        fn=search_papers,
        inputs=[topic, num],
        outputs=[result_box, export_dropdown]
    )
    export_btn.click(
        fn=export_results,
        inputs=export_dropdown,
        outputs=file_output
    )

# ===== å¯åŠ¨æœåŠ¡ =====
if __name__ == "__main__":
    logger.info("å¯åŠ¨ ScholarSift æœåŠ¡...")
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        debug=True,
        favicon_path=None  # å¯æ›¿æ¢ä¸ºä½ çš„ favicon.ico
    )